#!/usr/bin/env python3
"""
List BATS jobs on o.s.d & o3
"""

import argparse
import os
import sys
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from urllib.parse import urlparse

from lxml import html
import requests
from requests.exceptions import RequestException

from bats.repos import REPOS, find_file, grep_tarball


TIMEOUT = 30

session = requests.Session()


@dataclass
class Job:
    """
    Job class
    """

    name: str
    result: str
    results: list[dict]
    url: str


def get_job_id(url: str) -> int | None:
    """
    Get job ID from URL with no job ID in URL
    """
    try:
        got = session.get(url, timeout=TIMEOUT)
        got.raise_for_status()
    except RequestException as error:
        print(f"ERROR: {url}: {error}", file=sys.stderr)
        return None
    tree = html.fromstring(got.text)
    elem = tree.xpath('//*[@id="info_box"]/div[1]/i/a')
    return int(elem[0].text_content().removeprefix("#"))  # type: ignore


def get_job(url_string: str, verbose: bool = False) -> Job | None:
    """
    Get a job
    """
    if not url_string.startswith(("http:", "https:")):
        url_string = f"https://{url_string}"
    url = urlparse(url_string)

    if url.query:
        job_id = get_job_id(url_string)
        if job_id is None:
            return None
    else:
        job_id = int(os.path.basename(url.path).removeprefix("t"))

    api_url = f"{url.scheme}://{url.netloc}/api/v1/jobs/{job_id}"
    if verbose:
        api_url = f"{api_url}/details"
    try:
        got = session.get(api_url, timeout=TIMEOUT)
        got.raise_for_status()
    except RequestException as error:
        print(f"ERROR: {api_url}: {error}", file=sys.stderr)
        return None
    info = got.json()["job"]

    return Job(
        name=info["name"],
        result=info["result"] if info["result"] != "none" else info["state"],
        results=info.get("testresults") if info["result"] == "failed" else None,
        url=f"{url.scheme}://{url.netloc}/tests/{job_id}",
    )


def get_urls(repos: list[str]) -> list[str]:
    """
    Get URL's from YAML schedules
    """

    def process_repo(repo: str) -> list[str]:
        urls = []
        for file in grep_tarball(repo, "*.yaml"):
            for product in find_file(file):
                urls.append(product.url)
        return urls

    urls = []
    with ThreadPoolExecutor(max_workers=min(10, len(repos))) as executor:
        for result in executor.map(process_repo, repos):
            urls.extend(result)
    return urls


def main() -> None:
    """
    Main function
    """
    parser = argparse.ArgumentParser(
        prog="bats_jobs",
        description="list BATS jobs in o.s.d & o3",
    )
    parser.add_argument("-v", "--verbose", action="store_true")
    parser.add_argument("urls", nargs="*", metavar="url")
    args = parser.parse_args()

    if not args.urls:
        args.urls = get_urls(REPOS)

    with ThreadPoolExecutor(max_workers=min(10, len(args.urls))) as executor:
        for job in executor.map(lambda u: get_job(u, args.verbose), args.urls):
            if job is None:
                continue
            job.result = job.result.upper() if job.result == "failed" else job.result
            print(f"{job.result:10}  {job.url:<42}  {job.name}")
            # Skip non-failed jobs
            if job.result != "FAILED" or not job.results:
                continue
            for result in job.results:
                # Skip non-failed modules
                if result["result"] == "failed":
                    if not result["has_parser_text_result"]:
                        print(f"\t{result['name']}")
                        continue
                    for test in result["details"]:
                        # Skip non-failed sub-tests
                        if test["result"] == "fail":
                            print(f"\t{result['name']:<30}  {test['text_data']}")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        sys.exit(1)
    finally:
        session.close()
